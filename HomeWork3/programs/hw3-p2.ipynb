{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "1. Add your name and HW Group Number below.\n",
    "2. Complete each question. Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", and delete and `throw NotImplementedError()` lines.\n",
    "3. Where applicable, run the test cases *below* each question to check your work. **Note**: In addition to the test cases you can see, the instructor may run additional test cases, including using *other datasets* to validate you code.\n",
    "4. Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). You can also use the **Validate** button to run all test cases.\n",
    "5. Turn in your homework by going to the main screen in JupyterHub, clicking the Assignments menu, and submitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nName: Vishnu Challa\\nHW Group Number: 40\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Name: Vishnu Challa\n",
    "HW Group Number: 40\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5ad03ee509d4eb8c8e9ca5b808ef4d0",
     "grade": false,
     "grade_id": "cell-13c5ff854ff08180",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## HW3: Artifical Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f8fea1ca998c71136b134971c940e64",
     "grade": false,
     "grade_id": "cell-58e7d9549befcb26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1) Neural Network Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1afc26595edc821ee222b3ca6b5d740c",
     "grade": false,
     "grade_id": "cell-b5a9ea2f81e0f5ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "First, go to Tensorflow's [Neural Network Playground](https://playground.tensorflow.org/). This website is an interactive and exploratory visualization of how the features, number of layers, training time, etc, influence the classification boundries of an ANN. Right now, we'll only worry ourselves with *classification* problems.\n",
    "\n",
    "Play with the visualization, and then answer the following questions below.\n",
    "\n",
    "#### Scenarios\n",
    "\n",
    "1. Using the default network topology, try training the network with the different activation functions (ReLU, Tanh, Sigmoid, Linear). What effect does the activation function have on the training time? What effect does the activation function have on the shape of the classification boundries?\n",
    "2. Take a look at [this setup](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=2,2&seed=0.21855&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false). Train until the classification boundry converges. This is one of the rare cases where the nodes in an ANN can be (semi) interpreted. What do the nodes in the first hidden layer represent? What about the second hidden layer? How do you think the ANN uses these learned \"features\" to make a decision?\n",
    "\n",
    "#### Exploration\n",
    "For each of the following questions:\n",
    "* Make a prediction before you begin exploring and testing.\n",
    "* Include a link to your scenario.\n",
    "* Explain why you think this scenario has this property.\n",
    "\n",
    "**Questions**\n",
    "\n",
    "3. Find a scenario where a simple model (fewer neurons) outperforms a complex model. (In regards to overfitting)\n",
    "4. Find a scenario where no hidden layers perform well.\n",
    "5. Find a scenario where a model with no hidden layers performs poorly no matter the features.\n",
    "6. Find a scenario where it takes a lot of training time to get a correct solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0bcc124eadc6eb162c540d577f6b4eda",
     "grade": true,
     "grade_id": "ANN-explore",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. From the default network topology, among all the activations functions the performance order would be ReLU > Tanh > Sigmoid > Linear i.e. ReLU performs the best and Linear is the least performing one. The timing taken by each function is as below: <br>\n",
    "ReLU = 000,355 Epochs <br>\n",
    "Tanh = 000,530 Epochs <br>\n",
    "Sigmoid = 005,030 Epochs <br>\n",
    "Linear = Infinite Epochs <br>\n",
    "And also Linear function performs better on the linearly separable data. Tanh and Sigmoid perform almost same on any kind of randomly scattered data. Whereas ReLU performs like a balance between both Linear and Simoid, Tanh functions on any kind of random shaped data.<br><br>\n",
    "2. The first hidden layer sets the initial linear boundaries to classify the data. And the second hidden layer generates the linear boundaries based on the output of the first hidden layer and tries to optimize the model further in order to make better decisions on the test data. ANN uses the learned feature to adjust the weights on the input data and tries to adjust them further leading to minimum training loss. This might lead to overfitting in some cases. So there must always be some limit in the number of epochs during training phase, so that we can finalize the model in a balanced way to further use it for predictions. <br><br>\n",
    "3. In case of linearly separable data if we feed a complex model with only one neuron in the first hidden layer and then pass it to further layers with more number of neurons then the model takes infinite time for the training.<br>\n",
    "   Simple model: https://tinyurl.com/bddp4yaw <br>\n",
    "   Complex model: https://tinyurl.com/yckm5kvv <br>\n",
    "   Both the models use Sigmoid function and simple model outperforms the complex one when linearly separable data is fed as input.<br>\n",
    "   This is beacause in the complex model one neuron in first hidden layer sets an initial linear boundary and the further layers take more time in setting linear boundaries on top of the previous ones. That is why the complex model performs bad.<br><br>\n",
    "4. In case of no hidden layers for linear separable data any simple model performs better than any ANN which has hidden layers.\n",
    "   Model with no hidden layers: https://tinyurl.com/ztere6cx <br>\n",
    "   Model with hidden layers: https://tinyurl.com/ntkccp4v <br>\n",
    "   The model with no hidden layers performs better because it is easy to set a boundary for linear separable data. By increasing number of hidden layers the model gets complex uneccesaryly trying to set more boundaries.<br><br>\n",
    "5. In case of no hidden layers for randomly shaped data, we need a hidden layer in order to perform the predictions when ReLU is used as an activation function.<br>\n",
    "   Model with no hidden layers: https://tinyurl.com/4bem4v23 <br>\n",
    "   Model with hidden layers: https://tinyurl.com/bdfskk4s <br>\n",
    "   In the above case the model with hidden layers performed better because the data input was complex shaped and the hidden layers make use of activation functions to set the boundaries and adjust the weights on the all the input features. So it is easier to classify with ANN having hidden layers in this case.<br><br>\n",
    "6. In case of a model with randomly shaped data, for a simple neural network using RELU activation takes a lot of time to get trained.<br>\n",
    "   Model: https://tinyurl.com/35u8jw35<br>\n",
    "   In the above case the model takes inifinte time to get trained because the input data is randomly shaped and the activation function ReLU continously tries to set the boundaries which bring no change to the updated weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06690a70f1fefb3ed96e1c736d741d81",
     "grade": false,
     "grade_id": "cell-e9233095f9bf0595",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2) Training and Testing a Neural Network (Group)\n",
    "\n",
    "For this problem, you'll be looking at a subset of the [UCI ML hand-written digits datasets](https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits), which contains images of hand-written digits: 10 classes where each class refers to a digit.\n",
    "\n",
    "Each data entry is a input matrix of 8x8 where each element is an integer in the range 0..16. The matrix is flattened in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5252d6b1cc40895ec7e5e1fe2f72c1b4",
     "grade": false,
     "grade_id": "cell-2add9a421651bb2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For this question, **you have enough experience to do the entire model pipeline yourself**. That means *loading the data, creating splits, scaling the data, training and tuning the model, and evaluating the model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf2a2867d4065882599c260b7e79e054",
     "grade": false,
     "grade_id": "cell-72eba8ac84354b45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58caa9d8e815c701f5062c15266523bf",
     "grade": false,
     "grade_id": "cell-5c9b6a78fc7c131d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 1: Load the data. Use `np.unique()` to check the class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "df = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "237d4d5d3975f1344130eeae4debcf5c",
     "grade": false,
     "grade_id": "cell-71b61c2fc389a33f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "       [ 0.,  0., 10., ..., 12.,  1.,  0.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3056f3fd2a14ac78a45fcd487a9552e6",
     "grade": false,
     "grade_id": "cell-cdddfe25259e2bea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " array([178, 182, 177, 183, 181, 182, 181, 179, 174, 180]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a distribution of the class label (target)\n",
    "np.unique(df.target, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Split the data into X (feautres) and Y (class)\n",
    "\n",
    "Assign the variables below to split the dataset in to X (features) and Y (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a97e24d17590ae6a7617edc4a905ba13",
     "grade": false,
     "grade_id": "slip-feature-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = Y = None\n",
    "X = df.data\n",
    "Y = df.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f08e7a67e744709376803a6944f1e302",
     "grade": false,
     "grade_id": "cell-057d09a1cf1ca024",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 3: Create your train/test split. Use the provided random_state.\n",
    "\n",
    "**Note**: You should use a `train_size` of 0.1, or 10%. Normally we would want to use more of our data for training, but since ANNs are computationally expensive, we're keeping the training dataset small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61f94b1c8f1dbdb65547d3b8c5438dda",
     "grade": false,
     "grade_id": "slit-train-test",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = X_test = y_train = y_test = None\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.1, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd13cc189ed658f60008562a71c8bbd2",
     "grade": true,
     "grade_id": "slit-train-test_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert X_train.shape == (179, 64)\n",
    "assert y_train.shape == (179, )\n",
    "assert X_test.shape == (1618, 64)\n",
    "assert y_test.shape == (1618, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "861bb2149ea72750302e3aa911d0b932",
     "grade": false,
     "grade_id": "cell-ed20d5db7f3ddc72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 4: Use a [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to normalize the image data. \n",
    "\n",
    "Pixel data, like other data we've encountered, should often be scaled before classification. While in practice scaling image data can be more complex, in this exercise we'll continue to use the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "\n",
    "Fit the scaler only the the training X features, and then apply it to both training and test X features. We do this because in practice, we wouldn't be able to see data in the test X, so it shouldn't affect feature transformation. We therefore only use X_train for feature transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a26cd46ecd7c2f77e81624022d919bfc",
     "grade": false,
     "grade_id": "scale-data",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assign these variables the standardized training and test datasets\n",
    "X_stand_train = X_stand_test = None\n",
    "scaler = StandardScaler()\n",
    "X_stand_train = scaler.fit_transform(X_train)\n",
    "X_stand_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d84abf9a26f19e233c6e1185e6fb9c4",
     "grade": true,
     "grade_id": "scale-data_test",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_zero_indices = [0, 7, 23, 31, 32, 39]\n",
    "test_zero_indices = [0, 32, 39]\n",
    "\n",
    "for i in range(X_stand_train.shape[1]):\n",
    "    if i in train_zero_indices:\n",
    "        np.testing.assert_almost_equal(np.mean(X_stand_train[:, i]), 0)\n",
    "        np.testing.assert_almost_equal(np.std(X_stand_train[:, i]), 0)\n",
    "    else:\n",
    "        np.testing.assert_almost_equal(np.mean(X_stand_train[:, i]), 0)\n",
    "        np.testing.assert_almost_equal(np.std(X_stand_train[:, i]), 1)\n",
    "        \n",
    "    if i in test_zero_indices:\n",
    "        np.testing.assert_almost_equal(np.mean(X_stand_test[:, i]), 0)\n",
    "        np.testing.assert_almost_equal(np.std(X_stand_test[:, i]), 0)\n",
    "    else:\n",
    "        assert np.mean(X_stand_test[:, i]) != 0\n",
    "        assert np.std(X_stand_test[:, i]) != 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce78e40d4906bc00582a65bf3f16b72b",
     "grade": false,
     "grade_id": "cell-6451bd16b481009b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 5:  Train an MLP with default hyperparameters.\n",
    "\n",
    "For the following, you'll be using sklearn's built in Multi-layer Perceptron classifier [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).\n",
    "\n",
    "Use the default hyperparams aside from `max_iter`. `max_iter` is how many iterations of training the ANN goes though until it manually stops. The default `max_iter=200` is too long for our data currently. \n",
    "\n",
    "**Use random_state as the random_states and max_iter=20**. The detault parameters will use a single hidden layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "368b885315c15dcc4279fcde9f301205",
     "grade": false,
     "grade_id": "cell-4ac9e87863d5d366",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b41a09cb52b81afc186a8576abdf99e",
     "grade": false,
     "grade_id": "mlp-train",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "clf=None\n",
    "clf = MLPClassifier(random_state=random_state, max_iter=20).fit(X_stand_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13d7d392c4d1fe1175786f563400c21e",
     "grade": false,
     "grade_id": "cell-2722a12a4e946191",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 6:  Evaluate the model on the test dataset using a confusion matrix and a classification report\n",
    "\n",
    "Like all classifiers, the MLP has a [`predict`](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier.predict) function that is used to make predictions on trianing or test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f96fdbe58f2c6ec3837b14f164b796b",
     "grade": false,
     "grade_id": "cell-5292fd4b6c7cdc73",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a40132caf743b6cce22b5ab28a27a68",
     "grade": false,
     "grade_id": "mlp-confusion-matrix",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[146,   0,   3,   0,   3,   0,   0,   0,   1,   0],\n",
       "       [  1,  87,  27,   4,  12,   5,   3,   2,  20,   0],\n",
       "       [  0,   4, 127,   6,   2,   1,   4,   0,  10,   5],\n",
       "       [  0,   2,  19, 115,   0,   2,   2,   0,   9,  17],\n",
       "       [  6,   2,   3,   2, 135,   7,   0,   3,   1,   3],\n",
       "       [  7,   3,  12,   0,   1, 133,   1,   0,   3,   2],\n",
       "       [ 21,   0,  82,   0,  12,   6,  35,   0,  11,   2],\n",
       "       [  0,   5,   1,  45,   2,  13,   0,  91,  10,   0],\n",
       "       [  7,  10,  21,   5,  12,  21,   2,   0,  53,  21],\n",
       "       [ 10,   2,  36,  13,   5,  17,   1,   4,  28,  51]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the classifier and assign mlp_cm to the confusion matrix of the evaluation\n",
    "mlp_cm = None\n",
    "y_pred = clf.predict(X_stand_test)\n",
    "mlp_cm = confusion_matrix(y_test, y_pred)\n",
    "mlp_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6dfa72cc746abc62b5bb84494a5c0579",
     "grade": true,
     "grade_id": "mlp-confusion-matrix_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(mlp_cm, [[146,   0,   3,   0,   3,   0,   0,   0,   1,   0],\n",
    "       [  1,  87,  27,   4,  12,   5,   3,   2,  20,   0],\n",
    "       [  0,   4, 127,   6,   2,   1,   4,   0,  10,   5],\n",
    "       [  0,   2,  19, 115,   0,   2,   2,   0,   9,  17],\n",
    "       [  6,   2,   3,   2, 135,   7,   0,   3,   1,   3],\n",
    "       [  7,   3,  12,   0,   1, 133,   1,   0,   3,   2],\n",
    "       [ 21,   0,  82,   0,  12,   6,  35,   0,  11,   2],\n",
    "       [  0,   5,   1,  45,   2,  13,   0,  91,  10,   0],\n",
    "       [  7,  10,  21,   5,  12,  21,   2,   0,  53,  21],\n",
    "       [ 10,   2,  36,  13,   5,  17,   1,   4,  28,  51]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "658b2fda957cc113c1ea4a3997ea542e",
     "grade": false,
     "grade_id": "mlp-report",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.95      0.83       153\n",
      "           1       0.76      0.54      0.63       161\n",
      "           2       0.38      0.80      0.52       159\n",
      "           3       0.61      0.69      0.65       166\n",
      "           4       0.73      0.83      0.78       162\n",
      "           5       0.65      0.82      0.72       162\n",
      "           6       0.73      0.21      0.32       169\n",
      "           7       0.91      0.54      0.68       167\n",
      "           8       0.36      0.35      0.36       152\n",
      "           9       0.50      0.31      0.38       167\n",
      "\n",
      "    accuracy                           0.60      1618\n",
      "   macro avg       0.64      0.60      0.59      1618\n",
      "weighted avg       0.64      0.60      0.59      1618\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Similarly generate a classification report for the test dataset\n",
    "mlp_clf_report = None\n",
    "mlp_clf_report = classification_report(y_test, y_pred)\n",
    "print(mlp_clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb0aef0c1ddf44f125f4543b64075d03",
     "grade": true,
     "grade_id": "mlp-report_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert mlp_clf_report == '              precision    recall  f1-score   support\\n\\n           0       0.74      0.95      0.83       153\\n           1       0.76      0.54      0.63       161\\n           2       0.38      0.80      0.52       159\\n           3       0.61      0.69      0.65       166\\n           4       0.73      0.83      0.78       162\\n           5       0.65      0.82      0.72       162\\n           6       0.73      0.21      0.32       169\\n           7       0.91      0.54      0.68       167\\n           8       0.36      0.35      0.36       152\\n           9       0.50      0.31      0.38       167\\n\\n    accuracy                           0.60      1618\\n   macro avg       0.64      0.60      0.59      1618\\nweighted avg       0.64      0.60      0.59      1618\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03b8b2ce1eecea4f6da7dfd5ad528b2f",
     "grade": false,
     "grade_id": "mlp-report-train",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94        25\n",
      "           1       0.82      0.67      0.74        21\n",
      "           2       0.67      0.89      0.76        18\n",
      "           3       0.74      0.82      0.78        17\n",
      "           4       0.78      0.95      0.86        19\n",
      "           5       0.79      0.95      0.86        20\n",
      "           6       0.50      0.25      0.33        12\n",
      "           7       0.88      0.58      0.70        12\n",
      "           8       0.63      0.55      0.59        22\n",
      "           9       0.64      0.54      0.58        13\n",
      "\n",
      "    accuracy                           0.75       179\n",
      "   macro avg       0.73      0.72      0.71       179\n",
      "weighted avg       0.75      0.75      0.74       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For comparison, generate a classification report for the *training* dataset\n",
    "mlp_clf_report = None\n",
    "x_pred = clf.predict(X_stand_train)\n",
    "mlp_clf_report = classification_report(y_train, x_pred)\n",
    "print(mlp_clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54aef820d468ca51899b71b4c23d5e6c",
     "grade": true,
     "grade_id": "mlp-report-train-public",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert mlp_clf_report == '              precision    recall  f1-score   support\\n\\n           0       0.89      1.00      0.94        25\\n           1       0.82      0.67      0.74        21\\n           2       0.67      0.89      0.76        18\\n           3       0.74      0.82      0.78        17\\n           4       0.78      0.95      0.86        19\\n           5       0.79      0.95      0.86        20\\n           6       0.50      0.25      0.33        12\\n           7       0.88      0.58      0.70        12\\n           8       0.63      0.55      0.59        22\\n           9       0.64      0.54      0.58        13\\n\\n    accuracy                           0.75       179\\n   macro avg       0.73      0.72      0.71       179\\nweighted avg       0.75      0.75      0.74       179\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66853a5470953dd03b0a4d212c5716cb",
     "grade": false,
     "grade_id": "cell-8915072f0111304b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "How well did the classifier do? What digit did it do best on? Which digits did it confuse the most? Do you think the classifier is likely over-fitting, underfitting or neither?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier did well. The model did best on predicting '0' based on the report metrics. The model got confused on predicting '6' based on the report metrics. I think the classifier has done neither over-fitting nor underfitting and performing in a balanced way. Becuase here the input data is fixed between a range 0-9 and the number of iterations performed in the neural network while traning should be good enough to train the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb6b4a3cadbefe8c0f2d1f4509236f49",
     "grade": false,
     "grade_id": "cell-5d631135dc57ee3e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3) Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "883a4d5752f8d13fc291122d57a2c01b",
     "grade": false,
     "grade_id": "cell-2525c741f9cb9464",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Hyperparams**:\n",
    "\n",
    "ANNs have *a lot* of hyperparams. This can include simple things such as the number of layers and nodes, up to tuning the learning rate and the gradient descent algorithm used. \n",
    "\n",
    "This process can require a lot of experimentation and intution through experience, but it can be automated to some extent using hyperparameter tuning. When we have multiple hyperparameters, we use an approach called GridSearch, where we try all combinations of various hyperparameters to find the combination that works best.\n",
    "\n",
    "For the following, you will practice the hyperparamater tuning for the [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) with sklearn's [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) function, you should explore different combination of the following parameters:\n",
    "\n",
    "* `activation`: The activation function of the the ANN. Defaults to ReLU.\n",
    "* `max_iter`: The ANN will train iterations until either the loss stops improving by a specified threshold, or `max_iters` is reached. Warning: the more you increase this, the more the training time will take! Patience is a virtue.\n",
    "* `hidden_layer_sizes`: A tuple representing the structure of the hidden layers. For example, giving the tuple `(100,50)` means that there's two hidden layers: the first being of size 100, and the second being of size 50. The tuple (100,) would mean a single hidden layer of size 100.\n",
    "\n",
    "Normally we would try many more possible combinations (and larger networks), but we've kept the list short to reduce computation time.\n",
    "\n",
    "**Try different permutations of these hyperprams and see how it affects the classification scores of your model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "756ecdaa3700f33e7a5908a6f17998f0",
     "grade": false,
     "grade_id": "cell-a25db6b11a11edba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# import the library\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1e16648a1d07009a43aa9be7d80dbd7",
     "grade": false,
     "grade_id": "cell-783ed3693fc71b1d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The parameter list you will explore\n",
    "parameters = {'activation':['logistic', 'relu'], 'max_iter':[5, 10], 'hidden_layer_sizes':[(50,),(20,)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e97d153088d3da3c26399d67c8c1ac41",
     "grade": false,
     "grade_id": "cell-e9071631cfc9a1dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now it's your turn, first initialize an MLPClassifier, make sure to **use \"random_state\" as the random_states**, then feed the parameter list defined above as well as the training data (**use \"X_stand_train\"**) to GridSearchCV to create a classifier with the best combination of the parameters. To do so, it uses cross-validation within the training dataset, so you never have to peek at your test dataset. Then fit the final classifier to the whole standardized training dataset.\n",
    "\n",
    "**Note**: You should use cv=2 in your grid search, to reduce the number of folds tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dac65bc8b3a7af88caf4be8f5aa65195",
     "grade": false,
     "grade_id": "hyper_tuning_ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, estimator=MLPClassifier(random_state=42),\n",
       "             param_grid={'activation': ['logistic', 'relu'],\n",
       "                         'hidden_layer_sizes': [(50,), (20,)],\n",
       "                         'max_iter': [5, 10]})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign clf to the optimized (with grid search) MLP model\n",
    "# TIP: Again, if you want to track the trianing progress, try passing \"verbose = True\" to the MLP\n",
    "clf = None\n",
    "mlp_classifier = MLPClassifier(random_state=random_state)\n",
    "clf = GridSearchCV(mlp_classifier, parameters, cv=2)\n",
    "clf.fit(X_stand_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, random_state=42)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's see the parameters of the winning model of our grid search\n",
    "# This model is the one clf actually uses when you call clf.fit\n",
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ace8d3e02041c5163417a7504814992",
     "grade": true,
     "grade_id": "hyper_tuning_test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert list(clf.cv_results_['rank_test_score']) == [5, 2, 7, 7, 4, 1, 6, 3]\n",
    "np.testing.assert_almost_equal(round(clf.best_score_,4), 0.2067)\n",
    "assert clf.best_params_['hidden_layer_sizes'] == (50,)\n",
    "assert clf.best_index_ == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ce06af1253691ac40513dd67e209168",
     "grade": false,
     "grade_id": "cell-b9e3ea0745f19e23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "833518aa8b1d696d074644f5dcee4ba3",
     "grade": false,
     "grade_id": "cell-3154d0ff3ab7d37a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now you will use the estimator with the best found parameters to generate predictions (stored as \"y_pred\") on testing dataset, **remember to use \"X_stand_test\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a86d5dd99019ac3275b8671fad7f8ca",
     "grade": false,
     "grade_id": "prediction_ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = None\n",
    "y_pred = clf.predict(X_stand_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e725c0742989907920c840afabcc74fe",
     "grade": true,
     "grade_id": "prediction_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert list(confusion_matrix(y_test,y_pred)[0]) == [18,  9,  2,  0, 22,  1,  1,  0,  1, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "198e7bb55f1e2bdbc74fe0737b06a4b2",
     "grade": false,
     "grade_id": "cell-a38625450c81cccb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.12      0.18       153\n",
      "           1       0.22      0.61      0.32       161\n",
      "           2       0.13      0.26      0.18       159\n",
      "           3       0.50      0.01      0.01       166\n",
      "           4       0.08      0.13      0.10       162\n",
      "           5       0.86      0.33      0.48       162\n",
      "           6       0.05      0.03      0.04       169\n",
      "           7       0.44      0.04      0.08       167\n",
      "           8       0.14      0.09      0.10       152\n",
      "           9       0.19      0.31      0.24       167\n",
      "\n",
      "    accuracy                           0.19      1618\n",
      "   macro avg       0.30      0.19      0.17      1618\n",
      "weighted avg       0.30      0.19      0.17      1618\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this toy example, we used a very limited set of hyperparmeters to reduce training time, and so our tuned model will actually do worse than our original. However, in practice, the tuned model should "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember**: Make sure to complete all problems (.ipynb files) in this assignment. When you finish, double-check the submission instructions at the top of this file, and submit on JupyterHub."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
